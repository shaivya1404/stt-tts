{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Whisper Fine-tuning with LoRA for Indic Languages\n",
        "\n",
        "This notebook fine-tunes Whisper for Indic languages using LoRA (Low-Rank Adaptation) for memory efficiency.\n",
        "\n",
        "**Features:**\n",
        "- LoRA fine-tuning (memory efficient)\n",
        "- Mixed precision (fp16)\n",
        "- Gradient checkpointing\n",
        "- WER evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q transformers datasets accelerate peft evaluate jiwer\n",
        "!pip install -q soundfile librosa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DATA_DIR = '/content/drive/MyDrive/indic_speech_data'\n",
        "OUTPUT_DIR = '/content/drive/MyDrive/indic_stt_models'\n",
        "\n",
        "import os\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Configuration\n",
        "MODEL_NAME = 'openai/whisper-medium'  # or whisper-small for faster training\n",
        "LANGUAGE = 'hi'  # hi, ta, te, bn, etc.\n",
        "BATCH_SIZE = 4\n",
        "LEARNING_RATE = 1e-5\n",
        "EPOCHS = 5\n",
        "MAX_STEPS = 5000\n",
        "\n",
        "# LoRA config\n",
        "LORA_R = 32\n",
        "LORA_ALPHA = 64\n",
        "LORA_DROPOUT = 0.1\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Model and Apply LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "# Load processor\n",
        "processor = WhisperProcessor.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Load model\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "model.config.use_cache = False\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "print(f\"Base model loaded: {MODEL_NAME}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply LoRA\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    target_modules=['q_proj', 'v_proj', 'k_proj', 'out_proj', 'fc1', 'fc2'],\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Audio\n",
        "import json\n",
        "\n",
        "def load_manifest_dataset(manifest_path):\n",
        "    \"\"\"Load dataset from JSONL manifest.\"\"\"\n",
        "    dataset = load_dataset('json', data_files={'data': manifest_path})['data']\n",
        "    dataset = dataset.cast_column('audio_filepath', Audio(sampling_rate=16000))\n",
        "    return dataset\n",
        "\n",
        "# Load datasets\n",
        "train_manifest = f\"{DATA_DIR}/manifests/stt_{LANGUAGE}_train.jsonl\"\n",
        "val_manifest = f\"{DATA_DIR}/manifests/stt_{LANGUAGE}_val.jsonl\"\n",
        "\n",
        "if os.path.exists(train_manifest):\n",
        "    train_dataset = load_manifest_dataset(train_manifest)\n",
        "    val_dataset = load_manifest_dataset(val_manifest) if os.path.exists(val_manifest) else None\n",
        "    print(f\"Train: {len(train_dataset)} samples\")\n",
        "    if val_dataset:\n",
        "        print(f\"Val: {len(val_dataset)} samples\")\n",
        "else:\n",
        "    print(f\"Manifest not found: {train_manifest}\")\n",
        "    print(\"Run notebook 01 first to prepare data.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_dataset(batch):\n",
        "    \"\"\"Prepare batch for training.\"\"\"\n",
        "    audio = batch['audio_filepath']\n",
        "    \n",
        "    # Compute input features\n",
        "    input_features = processor(\n",
        "        audio['array'],\n",
        "        sampling_rate=audio['sampling_rate'],\n",
        "        return_tensors='pt',\n",
        "    ).input_features[0]\n",
        "    \n",
        "    # Encode labels\n",
        "    labels = processor.tokenizer(batch['text']).input_ids\n",
        "    \n",
        "    return {\n",
        "        'input_features': input_features,\n",
        "        'labels': labels,\n",
        "    }\n",
        "\n",
        "# Process datasets\n",
        "train_processed = train_dataset.map(\n",
        "    prepare_dataset,\n",
        "    remove_columns=train_dataset.column_names,\n",
        ")\n",
        "\n",
        "if val_dataset:\n",
        "    val_processed = val_dataset.map(\n",
        "        prepare_dataset,\n",
        "        remove_columns=val_dataset.column_names,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: WhisperProcessor\n",
        "    \n",
        "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
        "        input_features = [{'input_features': f['input_features']} for f in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors='pt')\n",
        "        \n",
        "        label_features = [{'input_ids': f['labels']} for f in features]\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors='pt')\n",
        "        \n",
        "        labels = labels_batch['input_ids'].masked_fill(\n",
        "            labels_batch.attention_mask.ne(1), -100\n",
        "        )\n",
        "        \n",
        "        batch['labels'] = labels\n",
        "        return batch\n",
        "\n",
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import evaluate\n",
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "# Load WER metric\n",
        "wer_metric = evaluate.load('wer')\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "    \n",
        "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "    \n",
        "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "    \n",
        "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
        "    return {'wer': wer}\n",
        "\n",
        "# Training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=f\"{OUTPUT_DIR}/whisper_{LANGUAGE}_lora\",\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    max_steps=MAX_STEPS,\n",
        "    warmup_steps=500,\n",
        "    gradient_accumulation_steps=4,\n",
        "    fp16=True,\n",
        "    eval_strategy='steps',\n",
        "    eval_steps=500,\n",
        "    save_strategy='steps',\n",
        "    save_steps=500,\n",
        "    logging_steps=50,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=225,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='wer',\n",
        "    greater_is_better=False,\n",
        "    report_to=['tensorboard'],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_processed,\n",
        "    eval_dataset=val_processed if val_dataset else None,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")\n",
        "\n",
        "# Train\n",
        "print(\"Starting training...\")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate\n",
        "if val_dataset:\n",
        "    results = trainer.evaluate()\n",
        "    print(f\"\\nEvaluation Results:\")\n",
        "    print(f\"  WER: {results['eval_wer']:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test transcription\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "import IPython.display as ipd\n",
        "\n",
        "# Get a sample\n",
        "sample = train_dataset[0]\n",
        "audio = sample['audio_filepath']['array']\n",
        "sr = sample['audio_filepath']['sampling_rate']\n",
        "\n",
        "# Play audio\n",
        "print(f\"Ground truth: {sample['text']}\")\n",
        "ipd.Audio(audio, rate=sr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transcribe\n",
        "model.eval()\n",
        "\n",
        "input_features = processor(\n",
        "    audio,\n",
        "    sampling_rate=sr,\n",
        "    return_tensors='pt',\n",
        ").input_features.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    predicted_ids = model.generate(input_features, max_length=225)\n",
        "\n",
        "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
        "print(f\"Transcription: {transcription}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save LoRA adapters\n",
        "lora_path = f\"{OUTPUT_DIR}/whisper_{LANGUAGE}_lora_adapters\"\n",
        "model.save_pretrained(lora_path)\n",
        "print(f\"LoRA adapters saved to {lora_path}\")\n",
        "\n",
        "# Merge and save full model\n",
        "merged_model = model.merge_and_unload()\n",
        "merged_path = f\"{OUTPUT_DIR}/whisper_{LANGUAGE}_merged\"\n",
        "merged_model.save_pretrained(merged_path)\n",
        "processor.save_pretrained(merged_path)\n",
        "print(f\"Merged model saved to {merged_path}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
