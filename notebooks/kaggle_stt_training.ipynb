{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STT Model Training on Kaggle\n",
    "\n",
    "**What this notebook does:**\n",
    "1. Installs required packages\n",
    "2. Transcribes audio with Whisper\n",
    "3. Fine-tunes Whisper with LoRA\n",
    "4. Saves trained model\n",
    "\n",
    "**GPU Required:** Enable GPU in Settings ‚Üí Accelerator ‚Üí GPU P100 or T4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ùå GPU not enabled! Go to Settings ‚Üí Accelerator ‚Üí GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages (takes 2-3 minutes)\n",
    "!pip install -q transformers datasets accelerate peft\n",
    "!pip install -q librosa soundfile jiwer tensorboard\n",
    "!pip install -q openai-whisper\n",
    "!pip install -q bitsandbytes\n",
    "\n",
    "print(\"‚úÖ All packages installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Setup Paths\n",
    "\n",
    "**IMPORTANT:** Update the dataset path below to match your uploaded dataset name!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ============================================\n",
    "# UPDATE THIS PATH TO YOUR DATASET!\n",
    "# ============================================\n",
    "# Format: /kaggle/input/YOUR-DATASET-NAME\n",
    "DATASET_PATH = \"/kaggle/input/stt-training-data\"\n",
    "\n",
    "# Output path (Kaggle working directory)\n",
    "OUTPUT_PATH = \"/kaggle/working\"\n",
    "\n",
    "# Check if dataset exists\n",
    "if os.path.exists(DATASET_PATH):\n",
    "    print(f\"‚úÖ Dataset found at: {DATASET_PATH}\")\n",
    "    print(\"\\nContents:\")\n",
    "    for item in os.listdir(DATASET_PATH):\n",
    "        print(f\"  - {item}\")\n",
    "else:\n",
    "    print(f\"‚ùå Dataset not found at: {DATASET_PATH}\")\n",
    "    print(\"\\nAvailable datasets:\")\n",
    "    for item in os.listdir(\"/kaggle/input\"):\n",
    "        print(f\"  - /kaggle/input/{item}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Explore dataset structure (run this to see what's inside)\nimport os\n\ndef show_tree(path, prefix=\"\", max_depth=3, current_depth=0):\n    \"\"\"Show folder structure.\"\"\"\n    if current_depth >= max_depth:\n        return\n    \n    try:\n        items = sorted(os.listdir(path))\n        for i, item in enumerate(items[:10]):  # Limit to 10 items\n            item_path = os.path.join(path, item)\n            is_last = (i == len(items[:10]) - 1)\n            connector = \"‚îî‚îÄ‚îÄ \" if is_last else \"‚îú‚îÄ‚îÄ \"\n            \n            if os.path.isdir(item_path):\n                print(f\"{prefix}{connector}üìÅ {item}/\")\n                new_prefix = prefix + (\"    \" if is_last else \"‚îÇ   \")\n                show_tree(item_path, new_prefix, max_depth, current_depth + 1)\n            else:\n                print(f\"{prefix}{connector}üìÑ {item}\")\n        \n        if len(items) > 10:\n            print(f\"{prefix}    ... and {len(items) - 10} more items\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n\nprint(\"üìÇ Dataset Structure:\")\nprint(\"=\" * 50)\nshow_tree(DATASET_PATH)\nprint(\"=\" * 50)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "import whisper\nimport json\nfrom pathlib import Path\nfrom tqdm import tqdm\n\n# Load Whisper model for transcription\nprint(\"Loading Whisper medium model...\")\nmodel = whisper.load_model(\"medium\")\nprint(\"‚úÖ Model loaded!\")\n\ndef fix_audio_path(original_path, dataset_path):\n    \"\"\"Convert Windows path to Kaggle path (handles nested extracted folder).\"\"\"\n    path = original_path.replace('\\\\', '/')\n    \n    if 'extracted/' in path:\n        relative = path.split('extracted/')[-1]\n        # Handle nested extracted/extracted/ structure on Kaggle\n        return f\"{dataset_path}/extracted/extracted/{relative}\"\n    elif 'combined/' in path:\n        relative = path.split('combined/')[-1]\n        return f\"{dataset_path}/combined/{relative}\"\n    return original_path\n\ndef transcribe_manifest(manifest_path, lang, output_path):\n    \"\"\"Transcribe all audio files in a manifest.\"\"\"\n    \n    # Read manifest\n    with open(manifest_path, 'r', encoding='utf-8') as f:\n        lines = f.readlines()\n    \n    print(f\"Found {len(lines)} audio files to transcribe\")\n    \n    # Show sample path conversion\n    sample_entry = json.loads(lines[0].strip())\n    sample_path = sample_entry.get('audio_path') or sample_entry.get('audio_filepath')\n    fixed_sample = fix_audio_path(sample_path, DATASET_PATH)\n    print(f\"\\nPath conversion example:\")\n    print(f\"  Original: {sample_path}\")\n    print(f\"  Fixed:    {fixed_sample}\")\n    print(f\"  Exists:   {os.path.exists(fixed_sample)}\")\n    \n    if not os.path.exists(fixed_sample):\n        print(\"\\n‚ö†Ô∏è File not found! Let me search for audio files...\")\n        for root, dirs, files in os.walk(DATASET_PATH):\n            wav_files = [f for f in files if f.endswith('.wav')]\n            if wav_files:\n                print(f\"Found .wav files in: {root}\")\n                print(f\"Example file: {wav_files[0]}\")\n                break\n        return []\n    \n    print(\"\\n\")\n    results = []\n    not_found = 0\n    \n    for line in tqdm(lines, desc=f\"Transcribing {lang}\"):\n        entry = json.loads(line.strip())\n        original_path = entry.get('audio_path') or entry.get('audio_filepath')\n        \n        # Fix path for Kaggle\n        audio_path = fix_audio_path(original_path, DATASET_PATH)\n        \n        if os.path.exists(audio_path):\n            try:\n                result = model.transcribe(audio_path, language=lang)\n                entry['text'] = result['text'].strip()\n                entry['audio_filepath'] = audio_path\n                results.append(entry)\n            except Exception as e:\n                print(f\"Error transcribing {audio_path}: {e}\")\n        else:\n            not_found += 1\n            if not_found <= 3:\n                print(f\"File not found: {audio_path}\")\n    \n    if not_found > 3:\n        print(f\"... and {not_found - 3} more files not found\")\n    \n    print(f\"\\n‚úÖ Transcribed: {len(results)} files\")\n    print(f\"‚ùå Not found: {not_found} files\")\n    \n    # Save transcribed manifest\n    with open(output_path, 'w', encoding='utf-8') as f:\n        for entry in results:\n            f.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n    \n    print(f\"‚úÖ Saved to: {output_path}\")\n    return results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import whisper\nimport json\nfrom pathlib import Path\nfrom tqdm import tqdm\n\n# Load Whisper model for transcription\nprint(\"Loading Whisper medium model...\")\nmodel = whisper.load_model(\"medium\")\nprint(\"‚úÖ Model loaded!\")\n\ndef fix_audio_path(original_path, dataset_path):\n    \"\"\"Convert Windows path to Kaggle path.\"\"\"\n    # Handle Windows paths like: C:\\Users\\DELL\\Desktop\\...\\data\\stt\\extracted\\...\n    # Convert to: /kaggle/input/stt-training-data/extracted/...\n    \n    # Normalize path separators\n    path = original_path.replace('\\\\', '/')\n    \n    # Find the 'extracted' part and build Kaggle path\n    if 'extracted/' in path:\n        # Get everything after 'extracted/'\n        relative = path.split('extracted/')[-1]\n        return f\"{dataset_path}/extracted/{relative}\"\n    elif 'combined/' in path:\n        relative = path.split('combined/')[-1]\n        return f\"{dataset_path}/combined/{relative}\"\n    else:\n        return original_path\n\ndef transcribe_manifest(manifest_path, lang, output_path):\n    \"\"\"Transcribe all audio files in a manifest.\"\"\"\n    \n    # Read manifest\n    with open(manifest_path, 'r', encoding='utf-8') as f:\n        lines = f.readlines()\n    \n    print(f\"Found {len(lines)} audio files to transcribe\")\n    \n    # Show sample path conversion\n    sample_entry = json.loads(lines[0].strip())\n    sample_path = sample_entry.get('audio_path') or sample_entry.get('audio_filepath')\n    fixed_sample = fix_audio_path(sample_path, DATASET_PATH)\n    print(f\"\\nPath conversion example:\")\n    print(f\"  Original: {sample_path}\")\n    print(f\"  Fixed:    {fixed_sample}\")\n    print(f\"  Exists:   {os.path.exists(fixed_sample)}\\n\")\n    \n    results = []\n    not_found = 0\n    \n    for line in tqdm(lines, desc=f\"Transcribing {lang}\"):\n        entry = json.loads(line.strip())\n        original_path = entry.get('audio_path') or entry.get('audio_filepath')\n        \n        # Fix path for Kaggle\n        audio_path = fix_audio_path(original_path, DATASET_PATH)\n        \n        if os.path.exists(audio_path):\n            try:\n                result = model.transcribe(audio_path, language=lang)\n                entry['text'] = result['text'].strip()\n                entry['audio_filepath'] = audio_path\n                results.append(entry)\n            except Exception as e:\n                print(f\"Error transcribing {audio_path}: {e}\")\n        else:\n            not_found += 1\n            if not_found <= 3:  # Only show first 3 errors\n                print(f\"File not found: {audio_path}\")\n    \n    if not_found > 3:\n        print(f\"... and {not_found - 3} more files not found\")\n    \n    print(f\"\\n‚úÖ Transcribed: {len(results)} files\")\n    print(f\"‚ùå Not found: {not_found} files\")\n    \n    # Save transcribed manifest\n    with open(output_path, 'w', encoding='utf-8') as f:\n        for entry in results:\n            f.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n    \n    print(f\"‚úÖ Saved to: {output_path}\")\n    return results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Transcribe Hindi data\n# Check both possible locations for the manifest\nhi_manifest_options = [\n    f\"{DATASET_PATH}/combined/hi_train.jsonl\",\n    f\"{DATASET_PATH}/hi_train.jsonl\",\n]\n\nhi_manifest = None\nfor path in hi_manifest_options:\n    if os.path.exists(path):\n        hi_manifest = path\n        break\n\nif hi_manifest:\n    print(f\"Found Hindi manifest at: {hi_manifest}\")\n    hi_results = transcribe_manifest(\n        hi_manifest, \n        lang=\"hi\", \n        output_path=f\"{OUTPUT_PATH}/hi_train_transcribed.jsonl\"\n    )\nelse:\n    print(\"‚ùå Hindi manifest not found!\")\n    print(\"Checked locations:\")\n    for path in hi_manifest_options:\n        print(f\"  - {path}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Transcribe English data\nen_manifest_options = [\n    f\"{DATASET_PATH}/combined/en_train.jsonl\",\n    f\"{DATASET_PATH}/en_train.jsonl\",\n]\n\nen_manifest = None\nfor path in en_manifest_options:\n    if os.path.exists(path):\n        en_manifest = path\n        break\n\nif en_manifest:\n    print(f\"Found English manifest at: {en_manifest}\")\n    en_results = transcribe_manifest(\n        en_manifest, \n        lang=\"en\", \n        output_path=f\"{OUTPUT_PATH}/en_train_transcribed.jsonl\"\n    )\nelse:\n    print(\"‚ùå English manifest not found!\")\n    print(\"Checked locations:\")\n    for path in en_manifest_options:\n        print(f\"  - {path}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Prepare Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import librosa\n",
    "import numpy as np\n",
    "from datasets import Dataset, Audio\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# Load processor\n",
    "print(\"Loading Whisper processor...\")\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "def load_manifest(manifest_path):\n",
    "    \"\"\"Load manifest file and create dataset.\"\"\"\n",
    "    entries = []\n",
    "    with open(manifest_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            entry = json.loads(line.strip())\n",
    "            if entry.get('text'):  # Only include transcribed entries\n",
    "                entries.append({\n",
    "                    'audio': entry.get('audio_filepath') or entry.get('audio_path'),\n",
    "                    'text': entry['text']\n",
    "                })\n",
    "    return entries\n",
    "\n",
    "# Load transcribed data\n",
    "train_data = []\n",
    "\n",
    "hi_transcribed = f\"{OUTPUT_PATH}/hi_train_transcribed.jsonl\"\n",
    "en_transcribed = f\"{OUTPUT_PATH}/en_train_transcribed.jsonl\"\n",
    "\n",
    "if os.path.exists(hi_transcribed):\n",
    "    train_data.extend(load_manifest(hi_transcribed))\n",
    "    print(f\"Loaded {len(train_data)} Hindi samples\")\n",
    "\n",
    "if os.path.exists(en_transcribed):\n",
    "    en_data = load_manifest(en_transcribed)\n",
    "    train_data.extend(en_data)\n",
    "    print(f\"Loaded {len(en_data)} English samples\")\n",
    "\n",
    "print(f\"\\nTotal training samples: {len(train_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HuggingFace dataset\n",
    "from datasets import Dataset, Audio\n",
    "\n",
    "dataset = Dataset.from_list(train_data)\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "print(f\"Dataset created with {len(dataset)} samples\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    \"\"\"Prepare audio and text for training.\"\"\"\n",
    "    audio = batch[\"audio\"]\n",
    "    \n",
    "    # Process audio\n",
    "    batch[\"input_features\"] = processor(\n",
    "        audio[\"array\"], \n",
    "        sampling_rate=audio[\"sampling_rate\"],\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_features[0]\n",
    "    \n",
    "    # Process text\n",
    "    batch[\"labels\"] = processor.tokenizer(batch[\"text\"]).input_ids\n",
    "    \n",
    "    return batch\n",
    "\n",
    "# Process dataset\n",
    "print(\"Processing dataset (this may take a while)...\")\n",
    "processed_dataset = dataset.map(\n",
    "    prepare_dataset, \n",
    "    remove_columns=dataset.column_names,\n",
    "    num_proc=1\n",
    ")\n",
    "print(\"‚úÖ Dataset processed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Setup Model with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Load base model\n",
    "print(\"Loading Whisper small model...\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"‚úÖ Model ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Split inputs and labels\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"{OUTPUT_PATH}/whisper-finetuned\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-4,\n",
    "    warmup_steps=50,\n",
    "    num_train_epochs=3,\n",
    "    fp16=True,\n",
    "    logging_steps=25,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=processed_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training!\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(\"This will take 1-2 hours depending on your data size.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "model_save_path = f\"{OUTPUT_PATH}/whisper-stt-finetuned\"\n",
    "\n",
    "# Save LoRA weights\n",
    "model.save_pretrained(model_save_path)\n",
    "processor.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {model_save_path}\")\n",
    "print(\"\\nFiles saved:\")\n",
    "for f in os.listdir(model_save_path):\n",
    "    print(f\"  - {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample audio\n",
    "import torch\n",
    "\n",
    "def transcribe_audio(audio_path):\n",
    "    \"\"\"Transcribe audio using fine-tuned model.\"\"\"\n",
    "    # Load audio\n",
    "    audio, sr = librosa.load(audio_path, sr=16000)\n",
    "    \n",
    "    # Process\n",
    "    inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
    "    input_features = inputs.input_features.to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(input_features, max_length=225)\n",
    "    \n",
    "    # Decode\n",
    "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "    return transcription\n",
    "\n",
    "# Test with first audio file from dataset\n",
    "if len(train_data) > 0:\n",
    "    test_audio = train_data[0]['audio']\n",
    "    print(f\"Testing with: {test_audio}\")\n",
    "    print(f\"Original text: {train_data[0]['text']}\")\n",
    "    print(f\"Model output: {transcribe_audio(test_audio)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Download Model\n",
    "\n",
    "1. Click on the folder icon on the left sidebar\n",
    "2. Navigate to `/kaggle/working/whisper-stt-finetuned`\n",
    "3. Right-click ‚Üí Download\n",
    "\n",
    "Or create a zip file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create zip for easy download\n",
    "import shutil\n",
    "\n",
    "shutil.make_archive(\n",
    "    f\"{OUTPUT_PATH}/whisper-stt-model\",\n",
    "    'zip',\n",
    "    model_save_path\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model zipped: {OUTPUT_PATH}/whisper-stt-model.zip\")\n",
    "print(\"\\nDownload this file from the Output section on the right panel!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Done!\n",
    "\n",
    "Your trained model is saved in `/kaggle/working/whisper-stt-finetuned`\n",
    "\n",
    "To download:\n",
    "1. Go to **Output** tab on right panel\n",
    "2. Download `whisper-stt-model.zip`\n",
    "\n",
    "To use the model later, load with:\n",
    "```python\n",
    "from peft import PeftModel\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "\n",
    "base_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "model = PeftModel.from_pretrained(base_model, \"path/to/whisper-stt-finetuned\")\n",
    "processor = WhisperProcessor.from_pretrained(\"path/to/whisper-stt-finetuned\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}