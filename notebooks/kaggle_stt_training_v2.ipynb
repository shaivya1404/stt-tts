{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STT Model Training on Kaggle (Fixed & Tested)\n",
    "\n",
    "**Successfully tested: January 2026**\n",
    "\n",
    "**What this notebook does:**\n",
    "1. Installs required packages\n",
    "2. Transcribes audio with Whisper\n",
    "3. Fine-tunes Whisper with LoRA\n",
    "4. Saves trained model\n",
    "\n",
    "**GPU Required:** Settings â†’ Accelerator â†’ GPU P100 or T4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"âŒ Enable GPU: Settings â†’ Accelerator â†’ GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets accelerate peft\n",
    "!pip install -q librosa soundfile jiwer tensorboard\n",
    "!pip install -q openai-whisper bitsandbytes torchcodec\n",
    "print(\"âœ… Packages installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Setup Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ============================================\n",
    "# UPDATE THIS TO YOUR DATASET NAME!\n",
    "# ============================================\n",
    "DATASET_PATH = \"/kaggle/input/stt-training-data\"\n",
    "OUTPUT_PATH = \"/kaggle/working\"\n",
    "\n",
    "if os.path.exists(DATASET_PATH):\n",
    "    print(f\"âœ… Dataset found: {DATASET_PATH}\")\n",
    "    for item in os.listdir(DATASET_PATH):\n",
    "        print(f\"  - {item}\")\n",
    "else:\n",
    "    print(\"âŒ Dataset not found!\")\n",
    "    print(\"Available:\")\n",
    "    for item in os.listdir(\"/kaggle/input\"):\n",
    "        print(f\"  - /kaggle/input/{item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find .wav files location\n",
    "print(\"ðŸ” Searching for .wav files...\\n\")\n",
    "\n",
    "AUDIO_BASE_PATH = None\n",
    "for root, dirs, files in os.walk(DATASET_PATH):\n",
    "    wav_files = [f for f in files if f.endswith('.wav')]\n",
    "    if wav_files:\n",
    "        print(f\"âœ… Found {len(wav_files)} .wav files in:\")\n",
    "        print(f\"   {root}\")\n",
    "        AUDIO_BASE_PATH = root\n",
    "        break\n",
    "\n",
    "if not AUDIO_BASE_PATH:\n",
    "    print(\"âŒ No .wav files found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Transcribe Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Loading Whisper medium model...\")\n",
    "transcription_model = whisper.load_model(\"medium\")\n",
    "print(\"âœ… Model loaded!\")\n",
    "\n",
    "def fix_audio_path(original_path, dataset_path):\n",
    "    \"\"\"Convert Windows path to Kaggle path.\"\"\"\n",
    "    path = original_path.replace('\\\\', '/')\n",
    "    \n",
    "    if 'extracted/' in path:\n",
    "        relative = path.split('extracted/')[-1]\n",
    "        # Try nested (from ZIP upload)\n",
    "        nested = f\"{dataset_path}/extracted/extracted/{relative}\"\n",
    "        if os.path.exists(nested):\n",
    "            return nested\n",
    "        # Try regular\n",
    "        regular = f\"{dataset_path}/extracted/{relative}\"\n",
    "        if os.path.exists(regular):\n",
    "            return regular\n",
    "        return nested\n",
    "    elif 'combined/' in path:\n",
    "        relative = path.split('combined/')[-1]\n",
    "        return f\"{dataset_path}/combined/{relative}\"\n",
    "    return original_path\n",
    "\n",
    "def transcribe_manifest(manifest_path, lang, output_path):\n",
    "    \"\"\"Transcribe audio files from manifest.\"\"\"\n",
    "    with open(manifest_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    print(f\"Found {len(lines)} files\")\n",
    "    \n",
    "    # Test path\n",
    "    sample = json.loads(lines[0].strip())\n",
    "    sample_path = sample.get('audio_filepath') or sample.get('audio_path')\n",
    "    fixed = fix_audio_path(sample_path, DATASET_PATH)\n",
    "    print(f\"Path test: {os.path.exists(fixed)}\")\n",
    "    \n",
    "    if not os.path.exists(fixed):\n",
    "        print(f\"âŒ Path not found: {fixed}\")\n",
    "        return []\n",
    "    \n",
    "    results = []\n",
    "    for line in tqdm(lines, desc=f\"Transcribing {lang}\"):\n",
    "        entry = json.loads(line.strip())\n",
    "        orig_path = entry.get('audio_filepath') or entry.get('audio_path')\n",
    "        audio_path = fix_audio_path(orig_path, DATASET_PATH)\n",
    "        \n",
    "        if os.path.exists(audio_path):\n",
    "            try:\n",
    "                result = transcription_model.transcribe(audio_path, language=lang)\n",
    "                entry['text'] = result['text'].strip()\n",
    "                entry['audio_filepath'] = audio_path\n",
    "                results.append(entry)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for entry in results:\n",
    "            f.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    print(f\"âœ… Transcribed: {len(results)} files\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe Hindi\n",
    "hi_manifest = f\"{DATASET_PATH}/combined/hi_train.jsonl\"\n",
    "if os.path.exists(hi_manifest):\n",
    "    print(f\"Found: {hi_manifest}\")\n",
    "    hi_results = transcribe_manifest(hi_manifest, \"hi\", f\"{OUTPUT_PATH}/hi_train_transcribed.jsonl\")\n",
    "else:\n",
    "    print(\"Hindi manifest not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe English\n",
    "en_manifest = f\"{DATASET_PATH}/combined/en_train.jsonl\"\n",
    "if os.path.exists(en_manifest):\n",
    "    print(f\"Found: {en_manifest}\")\n",
    "    en_results = transcribe_manifest(en_manifest, \"en\", f\"{OUTPUT_PATH}/en_train_transcribed.jsonl\")\n",
    "else:\n",
    "    print(\"English manifest not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Process Data (Direct Method - Reliable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import librosa\n",
    "import torch\n",
    "from transformers import WhisperProcessor\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Loading processor...\")\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "# Load transcribed data\n",
    "train_data = []\n",
    "for filepath in [f\"{OUTPUT_PATH}/hi_train_transcribed.jsonl\", f\"{OUTPUT_PATH}/en_train_transcribed.jsonl\"]:\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                entry = json.loads(line.strip())\n",
    "                if entry.get('text') and entry.get('audio_filepath'):\n",
    "                    train_data.append(entry)\n",
    "\n",
    "print(f\"Loaded {len(train_data)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process audio files directly\n",
    "MAX_LABEL_LENGTH = 440\n",
    "\n",
    "processed_data = []\n",
    "errors = 0\n",
    "skipped = 0\n",
    "\n",
    "for entry in tqdm(train_data, desc=\"Processing\"):\n",
    "    audio_path = entry['audio_filepath']\n",
    "    \n",
    "    if not os.path.exists(audio_path):\n",
    "        errors += 1\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        audio, sr = librosa.load(audio_path, sr=16000)\n",
    "        input_features = processor(audio, sampling_rate=16000, return_tensors=\"pt\").input_features[0]\n",
    "        labels = processor.tokenizer(entry['text']).input_ids\n",
    "        \n",
    "        if len(labels) > MAX_LABEL_LENGTH:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        processed_data.append({'input_features': input_features.numpy(), 'labels': labels})\n",
    "    except:\n",
    "        errors += 1\n",
    "\n",
    "print(f\"\\nâœ… Processed: {len(processed_data)}\")\n",
    "print(f\"âŒ Errors: {errors}\")\n",
    "print(f\"â­ï¸ Skipped (long): {skipped}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "processed_dataset = Dataset.from_list(processed_data)\n",
    "print(f\"âœ… Dataset: {len(processed_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Setup Model with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "print(\"Loading Whisper small...\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "print(\"âœ… Model ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_features\": f[\"input_features\"]} for f in features]\n",
    "        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"{OUTPUT_PATH}/whisper-finetuned\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-4,\n",
    "    warmup_steps=50,\n",
    "    num_train_epochs=3,\n",
    "    fp16=True,\n",
    "    logging_steps=25,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=processed_dataset,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "print(\"âœ… Trainer ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸš€ Starting training...\")\n",
    "trainer.train()\n",
    "print(\"âœ… Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f\"{OUTPUT_PATH}/whisper-stt-finetuned\"\n",
    "model.save_pretrained(model_path)\n",
    "processor.save_pretrained(model_path)\n",
    "print(f\"âœ… Saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe(audio_path):\n",
    "    audio, sr = librosa.load(audio_path, sr=16000)\n",
    "    inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        ids = model.generate(inputs.input_features.to(model.device), max_length=225)\n",
    "    return processor.batch_decode(ids, skip_special_tokens=True)[0]\n",
    "\n",
    "if train_data:\n",
    "    test = train_data[0]\n",
    "    print(f\"Original: {test['text']}\")\n",
    "    print(f\"Model: {transcribe(test['audio_filepath'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.make_archive(f\"{OUTPUT_PATH}/whisper-stt-model\", 'zip', model_path)\n",
    "print(\"âœ… Download from Output panel â†’\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## How to Use Later\n",
    "\n",
    "```python\n",
    "from peft import PeftModel\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "import librosa, torch\n",
    "\n",
    "base = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "model = PeftModel.from_pretrained(base, \"./whisper-stt-finetuned\")\n",
    "processor = WhisperProcessor.from_pretrained(\"./whisper-stt-finetuned\")\n",
    "\n",
    "audio, sr = librosa.load(\"audio.wav\", sr=16000)\n",
    "inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
    "ids = model.generate(inputs.input_features, max_length=225)\n",
    "print(processor.batch_decode(ids, skip_special_tokens=True)[0])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
