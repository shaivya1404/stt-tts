{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# XTTS v2 Fine-tuning for Indic Languages\n",
        "\n",
        "This notebook fine-tunes XTTS v2 for Indic languages (Hindi, Tamil, Telugu, Bengali, etc.).\n",
        "\n",
        "**Requirements:**\n",
        "- Colab T4 GPU (16GB VRAM)\n",
        "- Google Drive for storage\n",
        "- Prepared manifests from notebook 01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q TTS torch torchaudio\n",
        "!pip install -q indic-nlp-library aksharamukha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set paths\n",
        "DATA_DIR = '/content/drive/MyDrive/indic_speech_data'\n",
        "OUTPUT_DIR = '/content/drive/MyDrive/indic_tts_models'\n",
        "\n",
        "import os\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load XTTS Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from TTS.tts.configs.xtts_config import XttsConfig\n",
        "from TTS.tts.models.xtts import Xtts\n",
        "from TTS.utils.manage import ModelManager\n",
        "\n",
        "# Configuration\n",
        "LANGUAGE = 'hi'  # Change as needed: hi, ta, te, bn\n",
        "BATCH_SIZE = 2\n",
        "LEARNING_RATE = 1e-5\n",
        "EPOCHS = 50\n",
        "GRADIENT_ACCUMULATION = 8\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load XTTS v2 model\n",
        "print(\"Loading XTTS v2 model...\")\n",
        "\n",
        "model_manager = ModelManager()\n",
        "model_path, config_path, _ = model_manager.download_model('tts_models/multilingual/multi-dataset/xtts_v2')\n",
        "\n",
        "config = XttsConfig()\n",
        "config.load_json(config_path)\n",
        "\n",
        "model = Xtts.init_from_config(config)\n",
        "model.load_checkpoint(config, checkpoint_dir=model_path)\n",
        "model = model.to(device)\n",
        "\n",
        "print(\"Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class XTTSDataset(Dataset):\n",
        "    def __init__(self, manifest_path, max_length=11.0, sample_rate=22050):\n",
        "        self.samples = []\n",
        "        self.max_length = max_length\n",
        "        self.sample_rate = sample_rate\n",
        "        \n",
        "        with open(manifest_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                if not line.strip():\n",
        "                    continue\n",
        "                sample = json.loads(line)\n",
        "                duration = sample.get('duration', 0)\n",
        "                if 1.0 <= duration <= max_length:\n",
        "                    self.samples.append(sample)\n",
        "        \n",
        "        print(f\"Loaded {len(self.samples)} samples\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        \n",
        "        # Load audio\n",
        "        try:\n",
        "            waveform, sr = torchaudio.load(sample['audio_filepath'])\n",
        "            if waveform.shape[0] > 1:\n",
        "                waveform = waveform.mean(dim=0, keepdim=True)\n",
        "            if sr != self.sample_rate:\n",
        "                resampler = torchaudio.transforms.Resample(sr, self.sample_rate)\n",
        "                waveform = resampler(waveform)\n",
        "            waveform = waveform.squeeze(0)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {sample['audio_filepath']}: {e}\")\n",
        "            waveform = torch.zeros(self.sample_rate)\n",
        "        \n",
        "        return {\n",
        "            'audio': waveform,\n",
        "            'text': sample['text'],\n",
        "            'language': sample.get('language', f'{LANGUAGE}-IN'),\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load datasets\n",
        "train_manifest = f\"{DATA_DIR}/manifests/tts_{LANGUAGE}_train.jsonl\"\n",
        "val_manifest = f\"{DATA_DIR}/manifests/tts_{LANGUAGE}_val.jsonl\"\n",
        "\n",
        "# Check if manifests exist\n",
        "if not os.path.exists(train_manifest):\n",
        "    print(f\"Train manifest not found: {train_manifest}\")\n",
        "    print(\"Please run notebook 01 first to prepare data.\")\n",
        "else:\n",
        "    train_dataset = XTTSDataset(train_manifest)\n",
        "    val_dataset = XTTSDataset(val_manifest) if os.path.exists(val_manifest) else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def collate_fn(batch):\n",
        "    max_len = max(item['audio'].shape[0] for item in batch)\n",
        "    audio_batch = []\n",
        "    audio_lengths = []\n",
        "    \n",
        "    for item in batch:\n",
        "        audio = item['audio']\n",
        "        audio_lengths.append(audio.shape[0])\n",
        "        if audio.shape[0] < max_len:\n",
        "            padding = torch.zeros(max_len - audio.shape[0])\n",
        "            audio = torch.cat([audio, padding])\n",
        "        audio_batch.append(audio)\n",
        "    \n",
        "    return {\n",
        "        'audio': torch.stack(audio_batch),\n",
        "        'audio_lengths': torch.tensor(audio_lengths),\n",
        "        'text': [item['text'] for item in batch],\n",
        "        'language': [item['language'] for item in batch],\n",
        "    }\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn,\n",
        "    pin_memory=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup optimizer\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=LEARNING_RATE,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "best_loss = float('inf')\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "    \n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "    \n",
        "    for batch_idx, batch in enumerate(pbar):\n",
        "        audio = batch['audio'].to(device)\n",
        "        audio_lengths = batch['audio_lengths'].to(device)\n",
        "        texts = batch['text']\n",
        "        languages = batch['language']\n",
        "        \n",
        "        try:\n",
        "            with autocast():\n",
        "                # Note: XTTS forward_train API may vary\n",
        "                # This is a simplified example\n",
        "                outputs = model.forward(\n",
        "                    texts[0],\n",
        "                    languages[0][:2],  # Use 2-char lang code\n",
        "                    audio[:1],  # Speaker reference\n",
        "                )\n",
        "                \n",
        "                # Simple reconstruction loss\n",
        "                loss = torch.nn.functional.mse_loss(\n",
        "                    outputs['wav'][:audio_lengths[0]],\n",
        "                    audio[0, :audio_lengths[0]]\n",
        "                )\n",
        "            \n",
        "            loss = loss / GRADIENT_ACCUMULATION\n",
        "            scaler.scale(loss).backward()\n",
        "            \n",
        "            if (batch_idx + 1) % GRADIENT_ACCUMULATION == 0:\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad()\n",
        "            \n",
        "            total_loss += loss.item() * GRADIENT_ACCUMULATION\n",
        "            num_batches += 1\n",
        "            \n",
        "            pbar.set_postfix({'loss': f\"{total_loss/num_batches:.4f}\"})\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error in batch {batch_idx}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    avg_loss = total_loss / max(num_batches, 1)\n",
        "    print(f\"Epoch {epoch+1} - Average Loss: {avg_loss:.4f}\")\n",
        "    \n",
        "    # Save checkpoint\n",
        "    if avg_loss < best_loss:\n",
        "        best_loss = avg_loss\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': avg_loss,\n",
        "        }, f\"{OUTPUT_DIR}/xtts_{LANGUAGE}_best.pt\")\n",
        "        print(f\"Saved best model with loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Test Synthesis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import IPython.display as ipd\n",
        "\n",
        "# Test text\n",
        "test_texts = {\n",
        "    'hi': 'नमस्ते, मेरा नाम है और मैं हिंदी बोल रहा हूं।',\n",
        "    'ta': 'வணக்கம், என் பெயர் தமிழ் மொழி பேசுகிறேன்.',\n",
        "    'te': 'నమస్కారం, నా పేరు తెలుగు భాషలో మాట్లాడుతున్నాను.',\n",
        "    'bn': 'নমস্কার, আমার নাম বাংলা ভাষায় কথা বলছি।',\n",
        "}\n",
        "\n",
        "model.eval()\n",
        "\n",
        "test_text = test_texts.get(LANGUAGE, test_texts['hi'])\n",
        "print(f\"Synthesizing: {test_text}\")\n",
        "\n",
        "# Get a reference audio from dataset\n",
        "ref_audio = train_dataset[0]['audio'].unsqueeze(0).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.synthesize(\n",
        "        test_text,\n",
        "        config,\n",
        "        speaker_wav=ref_audio,\n",
        "        language=LANGUAGE,\n",
        "    )\n",
        "\n",
        "# Play audio\n",
        "ipd.Audio(outputs['wav'], rate=22050)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Save Final Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save final model\n",
        "final_path = f\"{OUTPUT_DIR}/xtts_{LANGUAGE}_final\"\n",
        "os.makedirs(final_path, exist_ok=True)\n",
        "\n",
        "# Save model\n",
        "torch.save(model.state_dict(), f\"{final_path}/model.pt\")\n",
        "\n",
        "# Save config\n",
        "config.save_json(f\"{final_path}/config.json\")\n",
        "\n",
        "print(f\"Model saved to {final_path}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
